{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f736eae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3435285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"getting-started-with-openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c5c863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LangSmith project: getting-started-with-openai\n"
     ]
    }
   ],
   "source": [
    "print(\"Current LangSmith project:\", os.getenv(\"LANGCHAIN_PROJECT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e665966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\")\n",
    "result = llm.invoke(\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef35aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework designed to help developers build applications that leverage large language models (LLMs). It provides a comprehensive set of tools and abstractions to facilitate the integration of LLMs into various workflows, making it easier to create chatbots, question-answering systems, and other AI-driven applications.\n",
      "\n",
      "Key features of LangChain include:\n",
      "\n",
      "1. **Prompt Management:** Tools to structure, manage, and optimize prompts for better LLM performance.\n",
      "2. **Memory Management:** Components to handle conversational state and remember previous interactions.\n",
      "3. **Chains and Agents:** Abstractions that enable chaining multiple tasks or actions together, allowing for complex workflows.\n",
      "4. **Data Integration:** Seamless ways to connect with external data sources like APIs, databases, and files.\n",
      "5. **Output Parsing:** Utilities to interpret and process LLM outputs into usable formats.\n",
      "\n",
      "Overall, LangChain simplifies the development process of LLM-powered applications by providing modular, reusable components, enabling rapid development and experimentation.\n",
      "\n",
      "Would you like more detailed information on specific features or use cases?\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d697e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LangSmith project: getting-started-with-openai\n"
     ]
    }
   ],
   "source": [
    "print(\"Current LangSmith project:\", os.getenv(\"LANGCHAIN_PROJECT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca84ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are an expert AI Engineer. Provide me answers based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chat prompt templet\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"Your are an expert AI Engineer. Provide me answers based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04ae084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Certainly! **LangSmith** is a platform developed by **LangChain** designed to help developers build, debug, test, monitor, and deploy applications powered by Large Language Models (LLMs). It addresses many of the challenges faced when developing with LLMs by providing robust observability, dataset management, and evaluation tools.\\n\\n### Core Features\\n\\n1. **Debugging and Tracing:**\\n   - LangSmith allows you to trace your LLM-powered workflows, providing visibility into each step of your application's execution. You can thoroughly inspect prompts, inputs, outputs, intermediate steps, and errors to understand how your chains, agents, or tools are functioning.\\n\\n2. **Dataset Management and Testing:**\\n   - You can create and manage datasets of prompts and expected outputs. LangSmith supports running these datasets through your LLM chains or agents and comparing the results for regression and quality testing.\\n\\n3. **Custom Evaluation:**\\n   - LangSmith makes it easy to plug in custom evaluators, including LLM-as-a-judge, exact match, or semantic similarity metrics. You can automate the process of grading your model’s outputs and tracking their quality over time.\\n\\n4. **Real-time Monitoring:**\\n   - LangSmith provides dashboards to monitor your application’s performance, track usage, spot anomalies, and identify common failure modes as your app runs in production.\\n\\n5. **Collaboration and Sharing:**\\n   - You can share runs (traces), datasets, and reports with teammates, making it easier to work together and iterate more quickly.\\n\\n### Integration\\n\\n- **LangSmith** is tightly integrated with the **LangChain** Python (and JavaScript/TypeScript) libraries but can also be used independently through a simple API.\\n- You can log both synchronous and asynchronous LLM calls, whether using OpenAI, Anthropic, Cohere, or other providers, as well as your custom tools and prompts.\\n\\n### Typical Use Cases\\n\\n- Improving the reliability and quality of LLM applications (chatbots, data extraction, agents, etc.).\\n- Systematic evaluation as you tweak prompts, model parameters, or chain logic.\\n- Debugging complex multi-step workflows with visibility into every step.\\n- Monitoring production LLM applications for degradation or abnormal behavior.\\n\\n### Getting Started\\n\\n- **LangSmith** offers a free tier for developers. You can sign up at [www.langchain.com/langsmith](https://www.langchain.com/langsmith) and find documentation, SDKs, and integrations to get started with your projects.\\n\\n---\\n\\n**In summary:**  \\nLangSmith is like a “debugger + data lab + quality assurance” platform for LLM-powered apps, aiming to bring software engineering best practices to the rapidly evolving world of AI applications.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 547, 'prompt_tokens': 33, 'total_tokens': 580, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BvQFTRndpTA6NshogrYPRKRJzWTN6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--a0fdbeca-9949-46ad-a36d-7c1b98a29402-0' usage_metadata={'input_tokens': 33, 'output_tokens': 547, 'total_tokens': 580, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f35876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d487449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain=prompt|llm|output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6016d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! **LangSmith** is a developer tool and platform released by the team behind [LangChain](https://www.langchain.com/), designed specifically for monitoring, debugging, evaluating, and improving language model (LLM)-powered applications.\n",
      "\n",
      "### Key Features of LangSmith\n",
      "\n",
      "1. **Tracing and Logging:**\n",
      "   - LangSmith catches and records data about prompt inputs, LLM outputs, chain/tool invocations, and overall execution flow.\n",
      "   - It creates a detailed \"trace\" or history of each LLM or chain run, allowing developers to see step-by-step how their applications behave.\n",
      "\n",
      "2. **Evaluation:**\n",
      "   - It supports systematic evaluation of LLM outputs through metrics (e.g., accuracy, correctness) and manual labeling.\n",
      "   - You can use LangSmith to perform A/B testing, compare prompts, LLM models, or chains, to see what performs best.\n",
      "\n",
      "3. **Dataset Management:**\n",
      "   - LangSmith provides facilities to create, manage, and version datasets that can be used for evaluation and training.\n",
      "\n",
      "4. **Debugging:**\n",
      "   - Developers can inspect where outputs go awry and trace errors back to the relevant parts of their chain/tool integration or prompt logic.\n",
      "\n",
      "5. **Collaborative Interface:**\n",
      "   - It features a web dashboard that allows teams to visualize, annotate, and discuss runs and dataset evaluations.\n",
      "\n",
      "6. **Integration:**\n",
      "   - Integrates seamlessly with [LangChain](https://www.langchain.com/) (Python & JS/TS), but also works with general LLM flows via its API.\n",
      "   - Supports OpenAI, Azure, Anthropic, Cohere, and many other LLM providers.\n",
      "\n",
      "### Typical Use Cases\n",
      "\n",
      "- **Prompt Engineering:** Iteratively improve prompts by seeing how changing them affects answers.\n",
      "- **Chain/Agent Debugging:** Step through complex tool or agent flows to find logic or routing bugs.\n",
      "- **QA and Testing:** Ensure that changes to prompts, LLMs, or chains do not reduce performance.\n",
      "\n",
      "### How Is It Used?\n",
      "\n",
      "- You instrument your LangChain app with [LangSmith callbacks](https://docs.smith.langchain.com/docs/integrations/langchain-sdk).\n",
      "- Data is sent securely to your LangSmith project, where you can analyze runs, logs, traces, and evaluation results.\n",
      "\n",
      "### Why Is It Important?\n",
      "\n",
      "As LLM-powered apps become more complex, prompt debugging and pipeline evaluation is cumbersome via ad-hoc logging and notebooks. LangSmith centralizes observability and evaluation, making it much easier to maintain and improve production LLM applications.\n",
      "\n",
      "### Additional Info\n",
      "\n",
      "- **Official website:** [smith.langchain.com](https://smith.langchain.com/)\n",
      "- **Docs:** https://docs.smith.langchain.com/\n",
      "- **Open Beta Launch:** July 2023\n",
      "\n",
      "Let me know if you want a code example or more specific use case for LangSmith!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85cf8b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d48cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
